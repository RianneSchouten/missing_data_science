{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values Treatment in Data Science\n",
    "\n",
    "In an application-orientated field like data science, available datasets are almost always incomplete. Whether you like it or not, missing values treatment will become part of your analysis. \n",
    "\n",
    "Even in the situation where a data scientist quickly drops incomplete records from a dataset, a missing values treatment has (unconsciously) been chosen. Is it the most appropriate method, though? And will the analysis outcome be different when another missing data method is applied? In this blogpost, we present an interactive plot to explore the effect of missing data and missing data methods on the outcome of a data science pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data mechanisms\n",
    "\n",
    "In missing data theory, we classify missing data problems into three categories: MCAR, MAR and MNAR missingness. Consider a dataset with outcome feature Y, predicting features X and some variable Z that is not used in the analysis. For any dataset with missing values in a certain feature X, the data is: \n",
    "\n",
    "- Missing Completely At Random (MCAR) when every cell has a fixed, equal probability of being missing. As a result, both large and small data values will be missing. \n",
    "\n",
    "- Missing At Random (MAR) when the probability of being missing depends on the observed values of another feature X or Z. For instance, records with large values on 'Coarse aggr.' have a higher probability of being missing on feature 'Fly ash'. The correlation between 'Coarge aggr.' and 'Fly ash' defines which values of 'Fly ash' become missing. \n",
    "\n",
    "- Missing Not At Random (MNAR) when the probability of being missing depends on the unobserved, missing data. This can either be because suitable covariates for explaining missingness have not been recorded or are otherwise unavailable or when the probability of being missing depends on the missing X values itself. Extending the previous example, if the probability of measuring 'Coarse aggr.' varied according to 'Coarse aggr.' itself, this is missing not at random. \n",
    "\n",
    "When the missingness depends on the outcome variable Y, and when outcome variable Y is complete observed, scientific researchers would consider this a MAR situation. After all, Y can be used in imputation model. However, in data science models, it is generally not accepted to use the outcome variable in the model. Therefore, if missingness in X depends on outcome variable Y, it makes more sense to call this a MNAR mechanism. \n",
    "\n",
    "The figures below schematically show how the multivariate relationship between any variable Z and a feature X is affected by completely random missingness, missingness depending on Z and missingness depending on X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## add figures: Figures/MCAR.pdf, Figures/M(N)ARZ.pdf, Figures/M(N)ARX.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data methods\n",
    "\n",
    "There is an extensive amount of missing data methods available in several software packages. For our simulations, we use sklearn's data science pipeline. As such, it is easy to implement a method such as `dropna()` or to use the `mean` or `median` imputation functions from the `Imputer` class. \n",
    "\n",
    "But what are the effects of all these methods? In our simulations, we implemented the following six missing values treatments. \n",
    "\n",
    "- Listwise deletion: Drop incomplete rows from the dataset\n",
    "- Mean imputation: imputation with the column (i.e. feature) mean\n",
    "- Median imputation: imputation with the column (i.e. feature) median\n",
    "- Random imputation: imputation with a randomly chosen observed value (per column)\n",
    "- Regression imputation: each column (i.e. feature) is regressed on the other features. We predict each incomplete value by using the observed values of the other features. When a predictive value is unobserved, we mean impute first. \n",
    "- Stochastic regression imputation: regression imputation as explained above, but with the addition of some noise. To each imputation, we add (or subtract) a random value. We sample the noise from a normal distribution with mean 0 and standard deviation equal to the uncertainty of the regression imputation model. \n",
    "\n",
    "As sklearn has no other methods than mean, median and most frequent imputation, we made three custom adaptations of the `.Imputer` class for the random, regression and stochastic regression imputation methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation setup\n",
    "\n",
    "To get an idea of the effect of missing data methods on the outcome of a data science use case, we analyzed four datasets. We performed the simulations with two real datasets and four simulated datasets. The import and generation of these datasets can be found in our github repository, as well as all other simulation code. \n",
    "\n",
    "The setup is as follows: \n",
    "\n",
    "- We start by splitting a given dataset into 60% trainingset and 40% testset. \n",
    "\n",
    "- We then generate missing values in the training and testset with function `delete_data`. We assign missing values to all X features, but the output variable remains complete. In each simulation round, we create a MCAR, MARX, MARZ, MNARX and MNARZ missingness mechanism. In case of MARX, the missingness in feature X_1 depends on the values of feature X_2, X_2 depends on X_3 and so on. For MNARX, missingness in X_1 depends on X_1, X_2 on the values of X_2, and so on. For MARZ and MNARZ, we create a variable Z that correlates with the outcome variable Y. Next, the values of Z will determine which values in X become missing. For instance, records with a large value on Z have a higher probability to be missing on X_1. In case of MARZ, we use Z in the imputation model. In case of MNARZ, we do not use Z to impute. Note that Z is never part of the analysis model. \n",
    "\n",
    "- We create several missingness percentages. We generate the missingness such that 5%, 10%, 15%, up to 55% of the records have at least 1 missing value. In general, 50% of the cells of all incomplete rows have a missing value.\n",
    "\n",
    "- Next, we apply the six missing data methods to each combination of missingness mechanism and missingness percentage. As said before, we apply `dropna()`, `mean` and `median` imputation with `.Imputer`, and `random`, `regression` and `stochastic` regression imputation with three custom functions. \n",
    "\n",
    "- The outcome variable Y is regressed on the X features with a `LinearRegression()` model from `sklearn.linear_model`. In particular, we apply a missing data method to the training set, fit the regression model on the training set, apply the same missing data method to the testset, and make the predictions with the fitted regression model. We evaluate our model by retrieving the following `sklearn.metrics`: `mean_squared_error`, sqrt(`mean_squared_error`), `mean_absolute_error` and `explained_variance`.\n",
    "\n",
    "- Every combination of missingness mechanism, missingness proportion and missing values treatment is repeated 1000 times for the real datasets and 10 times for the simulated datasets (more is not needed due to the large size of these datasets). We report the average and IQR of the evaluation metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation outcome\n",
    "\n",
    "The simulation outcomes show interesting results. All these results are presented in the interactive plot below. It is possible to inspect the outcomes by selecting the dataset, the missingness mechanism, the evaluation metric, the missing data method and whether you want to see the inter quartile range (IQR). A discussion of the output is given below the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add interactive plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concrete Slump Test\n",
    "\n",
    "The Concrete Slump Test dataset has 7 numerical features that are used to predict the slump in cm (https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/slump/). The error metrics for the original, complete dataset are as follows:\n",
    "\n",
    "- MSE: 62.5\n",
    "- RMSE: 7.8\n",
    "- MAE: 6.5\n",
    "- EV: 0.15\n",
    "\n",
    "Compared with these values, missing values induce an increase of MSE, RMSE and MAE and a decrease of EV. This result is obvious, because the missing data creates uncertainty in the data. For the same reason, the evaluation error metrics become worse when the percentage of missingness increases. It is interesting to see this effect is largest for the drop.na method. \n",
    "\n",
    "When we inspect the results for a MCAR mechanism, we find that mean and median imputation perform better than regression, stochastic regression and random imputation. The results from mean and median imputation greatly overlap, indicating that the variable distributions in this dataset are quite symmetric. \n",
    "\n",
    "It is apparent that using regression imputation instead of mean/median imputation does not improve the model. This is the situation for all missingness mechanisms. Moreover, when the missingness depends on a variable outside the data (Z), it is not of influence whether or not you use Z to esimate the imputation (MARZ versus MNARZ). This outcome is quite unexpected. One possible explanation might be that in our simulation, the missingness in training set is comparable to the missingness in testset. You could argue that, as a consequence, any mistake you make fitting the imputation model, will result in the same mistakes in the testset. As a result, it could be that the accuracy of predictions are not affected. Of course, more research on this has to be done. \n",
    "\n",
    "Overall, if missingness depends on variable Z, the model performs slightly better than with the other missingness mechanisms. This has to do with the fact that Z correlates with outcome variable Y. Obviously, there are X features that have the same or an even larger correlation with outcome variable Y. As a result, you could expect that MARX and MNARX should give the same results as MARZ and MNARZ. However, in case of M(N)ARX, the missingness depends on all X features while in case of M(N)ARZ, the missingness depends on only one variable. Therefore, the effect of a Z-like X variable might not turn up in the M(N)ARX outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forest Fires\n",
    "\n",
    "The Forest Fires data can be found at https://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/. Estimating the area of a forest fire on the complete dataset does not give good results when using a linear regression model. The evaluation error metrics are as follows: \n",
    "\n",
    "- MSE: 4160.6\n",
    "- RMSE: 50.39\n",
    "- MAE: 21.5\n",
    "- EV: -0.8\n",
    "\n",
    "It is clear that a linear regression model is not useful to predict the area of forest fire. The explained variance even drops below 0! It is probably for that reason, that regression imputation gives results comparable to mean and median imputation. The imputation model is just as bad as the analysis model, and is not able to make reasonable estimations of the missing values. As a result, the regression model is nothing more than mean imputation. \n",
    "\n",
    "Stochastic regression imputation turns towards random imputation. Apparently, the uncertainty of the regression imputation model is so worse that adding noise to the imputations equals picking a random value from the observed data. In other words, the amount of noise added to the regression imputations is that large that the imputations come close to random imputations. \n",
    "\n",
    "Interestingly, we find that it is important whether or not you use Z to estimate the imputations (compare MARZ with MNARZ)! The improvement we expected to see with the 'Slump Concrete Test' dataset, we see now: regression imputation performs better than mean imputation when the information about the missingness is in this predicting variable (Z). \n",
    "\n",
    "However, I should remark that it is disturbing to see the extent to which the model improves by using regression imputation. The MSE, RMSE and MAE values become much smaller than the outcome of the complete dataset. Could this be an example of overfitting? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated data\n",
    "\n",
    "Our simulation datasets have either a poor or rich correlation structure (see figures below). A continuous output variable is sampled by making a linear equation with the features, using a random weights vector, and by adding noise. We generated the output variable with two levels of noise: little and much. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Figures/poor.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-627a04f0f97b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Number of features'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Poor structure'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Figures/poor.png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbbox_inches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tight'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36msavefig\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1571\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_frameon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1572\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1573\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1575\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, **kwargs)\u001b[0m\n\u001b[0;32m   2250\u001b[0m                 \u001b[0morientation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2251\u001b[0m                 \u001b[0mbbox_inches_restore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2252\u001b[1;33m                 **kwargs)\n\u001b[0m\u001b[0;32m   2253\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2254\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[1;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_string_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mfilename_or_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Figures/poor.png'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKAAdGoOwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c3dc3764a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "df_poor = pd.read_table('Simulations/Data/custom_dataset_poor_little.txt', sep='\\t')\n",
    "fig = plt.figure()\n",
    "plt.hist(df_poor.corr().values.ravel())\n",
    "plt.xlabel('Correlation')\n",
    "plt.ylabel('Number of features')\n",
    "plt.title('Poor structure')\n",
    "fig.savefig('Simulations/Figures/poor.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rich = pd.read_table('Simulations/Data/custom_dataset_rich_little.txt', sep = '\\t')\n",
    "fig = plt.figure()\n",
    "plt.hist(df_rich.corr().values.ravel())\n",
    "plt.xlabel('Correlation')\n",
    "plt.ylabel('Number of features')\n",
    "plt.title('Rich structure')\n",
    "fig.savefig('Simulations/Figures/rich.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete datasets are quite perfect (despite of the added noise). All four datasets have an explained variance perfecentage of 99.99%. The MSE and MAE are 0.01 and 0.08 for the datasets with little noise and 24.6 and 4.0 for the datasets with much noise.\n",
    "\n",
    "The perfect characteristics of these datasets make that mean and median imputation exactly overlap. The same applies to regression and stochastic regression imputation. Furthermore, the drop.na method gives perfect results. The reason for this outcome is that, in particular, the rows with extreme values are removed from the dataset.\n",
    "\n",
    "Nonetheless, what we understand from these simulation is that the performance of regression imputation is clearly affected by the correlation structure of the data. With strong correlations between the X features, regression and stochastic regression imputation perform better than mean and median imputation. At the same time, regression imputation performs well when there is much noise in the outcome variable. Although these results may seem contradictory, it makes sense that in case of noise in the outcome variable, regression imputation can add structure to the data. \n",
    "\n",
    "Note that more time has to be spend on investigating how the correlation structure is exactly related to the performance of the imputation models. In addition, Rianne is working on relating prediction accuracy to imputation accuracy. After all, do we choose our missing data methods based on convenience, based on evaluation error metrics or do we wish to restore the original data as best as possible? \n",
    "\n",
    "For now, mean and median imputation seem to be stable imputation methods for data science use cases. They are fast, easy to implement and very important, they are not sensitive for leakage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact information\n",
    "\n",
    "Rianne Schouten, Missing Data Specialist, rianne.schouten@dpa.nl, rianneschouten.github.io\n",
    "\n",
    "Coen Seinen, Data Scientist, coen.seinen@bigdatarepublic.nl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
