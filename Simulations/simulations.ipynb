{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation script for exploration of missing data strategies\n",
    "\n",
    "In this script, we perform simulations to explore the effect of several missing data techniques on the outcome of a regression analysis. We work with two real, example datasets (imported with file: import_real_data.ipynb) and four custom datasets (created in the file: generate_data.ipynb). We c an amputation function and generated multiple forms of missing data. We deal with the missing data by the following missing data methods: drop, mean imputation, median imputation, regression imputation, stochastic regression imputation and random imputation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from scipy import sparse\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# for performing simulation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import explained_variance_score as ev\n",
    "from math import sqrt\n",
    "\n",
    "# for creating imputation functions in class Imputer\n",
    "from sklearn.preprocessing.imputation import Imputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.sparsefuncs import _get_median\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils.validation import FLOAT_DTYPES\n",
    "from sklearn.externals import six\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amputation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to delete entries of dataframe\n",
    "def delete_data(df_,mechanism,output_variable,missing_row_proportion=0.05):\n",
    "    \n",
    "    df = df_.copy()\n",
    "    index_ = df.index.values\n",
    "    all_columns = df.drop(output_variable,1).columns.tolist()\n",
    "    total_rows = int(len(df)*missing_row_proportion)\n",
    "    cells_per_column = int(total_rows / 2) #50% of cells in selected rows are made missing    \n",
    "    df['z'] = df[output_variable] + 2*np.random.randn(len(df))\n",
    " \n",
    "    if mechanism == 'MCAR': \n",
    "        \n",
    "        index_selected = np.random.choice(index_,total_rows,replace=False)           \n",
    "        for col in all_columns:\n",
    "            drop_index = np.random.choice(index_selected,cells_per_column,replace=False)\n",
    "            df[col].loc[drop_index] = None\n",
    "            \n",
    "    elif mechanism == 'MARZ':\n",
    "        \n",
    "        weights = df.loc[index_]['z'].values + abs(df.loc[index_]['z'].values.min()) + 0.05\n",
    "        weights = weights/weights.sum(axis=0)\n",
    "        index_selected = np.random.choice(index_,total_rows,p=weights,replace=False) \n",
    "        \n",
    "        #records with a large y value receive a large weight\n",
    "        weights = df.loc[index_selected]['z'].values+\\\n",
    "        abs(df.loc[index_selected]['z'].values.min()) + 0.05\n",
    "        weights = weights/weights.sum(axis=0)\n",
    "        for col in all_columns:\n",
    "            drop_index = np.random.choice(index_selected,size=cells_per_column,p=weights,replace=False)            \n",
    "            df[col].loc[drop_index] = None\n",
    "            \n",
    "    elif mechanism == 'MARX':\n",
    "        \n",
    "        index_selected = np.random.choice(index_,total_rows,replace=False) \n",
    "                \n",
    "        weights = []\n",
    "        for col in all_columns:            \n",
    "            ind = all_columns.index(col) + 1\n",
    "            if ind == len(all_columns):\n",
    "                ind = 0\n",
    "            x_col = all_columns[ind]\n",
    "            \n",
    "            #records with a large value on the next X feature receive a large weight for the current X feature\n",
    "            wgts = df.loc[index_selected][x_col].values+\\\n",
    "            abs(df.loc[index_selected][x_col].values.min()) + 0.05\n",
    "            wgts = wgts/wgts.sum(axis=0)\n",
    "            weights.append(wgts) \n",
    "        \n",
    "        for col in all_columns:\n",
    "            ind = all_columns.index(col)\n",
    "            drop_index = np.random.choice(index_selected,size=cells_per_column,p=weights[ind],replace=False)\n",
    "            df[col].loc[drop_index] = None\n",
    "            \n",
    "    elif mechanism == 'MNARZ':\n",
    "        \n",
    "        weights = df.loc[index_]['z'].values + abs(df.loc[index_]['z'].values.min()) + 0.05\n",
    "        weights = weights/weights.sum(axis=0)\n",
    "        index_selected = np.random.choice(index_,total_rows,p=weights,replace=False)\n",
    "                \n",
    "        #records with a large y value receive a large weight\n",
    "        weights = df.loc[index_selected]['z'].values+\\\n",
    "        abs(df.loc[index_selected]['z'].values.min()) + 0.05\n",
    "        weights = weights/weights.sum(axis=0) \n",
    "        for col in all_columns:\n",
    "            drop_index = np.random.choice(index_selected,size=cells_per_column,p=weights,replace=False)\n",
    "            df[col].loc[drop_index] = None\n",
    "                                                              \n",
    "    elif mechanism == 'MNARX':   \n",
    "        \n",
    "        index_selected = np.random.choice(index_,total_rows,replace=False) \n",
    "\n",
    "        for col in all_columns:\n",
    "            #records with a large value on a feature receive a large weight\n",
    "            weights = df.loc[index_selected][col].values+\\\n",
    "            abs(df.loc[index_selected][col].values.min()) + 0.05\n",
    "            weights = weights/weights.sum(axis=0) \n",
    "            drop_index = np.random.choice(index_selected,size=cells_per_column,p=weights,replace=False)\n",
    "            df[col].loc[drop_index] = None\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_train_test(data,test_size=0.4):\n",
    "    train, test = train_test_split(data.index.values,test_size=test_size)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_drop(train,test,X_train,y_train,X_test,y_test,model):\n",
    "    \n",
    "    if 'z' in X_train.columns:\n",
    "        train = train.drop('z', 1)\n",
    "        test = test.drop('z', 1)\n",
    "        X_train = X_train.drop('z', 1)\n",
    "        X_test = X_test.drop('z', 1)\n",
    "    \n",
    "    # this approach is to save the right y values for train and testset\n",
    "    train_na_free = train.dropna()\n",
    "    test_na_free = test.dropna()\n",
    "        \n",
    "    X_train_use = X_train[X_train.index.isin(train_na_free.index)]\n",
    "    y_train_use = y_train[y_train.index.isin(train_na_free.index)]    \n",
    "    \n",
    "    X_test_use = X_test[X_test.index.isin(test_na_free.index)]\n",
    "    y_test_use = y_test[y_test.index.isin(test_na_free.index)]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train_use)\n",
    "    X_train_use = pd.DataFrame(data=scaler.transform(X_train_use), columns=X_train_use.columns) \n",
    "    X_test_use = pd.DataFrame(data=scaler.transform(X_test_use), columns=X_test_use.columns)\n",
    "        \n",
    "    mod = model.fit(X_train_use,y_train_use)\n",
    "    pred = mod.predict(X_test_use)\n",
    "    \n",
    "    del scaler\n",
    "    \n",
    "    return {'mse': mse(y_test_use,pred),\n",
    "            'rmse': sqrt(mse(y_test_use,pred)),\n",
    "            'ev': ev(y_test_use,pred),\n",
    "            'mae': mae(y_test_use,pred),\n",
    "            'dif': mse(y_train_use,mod.predict(X_train_use)) - mse(y_test_use,pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create imputation function for random, regression and stochastic regression imputation\n",
    "\n",
    "zip = six.moves.zip\n",
    "map = six.moves.map\n",
    "\n",
    "__all__ = [\n",
    "    'Imputer',\n",
    "]\n",
    "\n",
    "def _get_mask(X, value_to_mask):\n",
    "    \"\"\"Compute the boolean mask X == missing_values.\"\"\"\n",
    "    if value_to_mask == \"NaN\" or np.isnan(value_to_mask):\n",
    "        return np.isnan(X)\n",
    "    else:\n",
    "        return X == value_to_mask\n",
    "    \n",
    "class custom_imputer(Imputer):\n",
    "       \n",
    "    def fit(self, X, y = None):\n",
    "        \"\"\"Fit the imputer on X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Input data, where ``n_samples`` is the number of samples and\n",
    "            ``n_features`` is the number of features.\n",
    "        Returns\n",
    "        -------\n",
    "        self : Imputer\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check parameters\n",
    "        allowed_strategies = [\"mean\", \"median\", \"most_frequent\", \"random\", \"regression\", \"stochastic\"] \n",
    "        if 'z' in X.columns:\n",
    "            self.remove = True\n",
    "        else:\n",
    "            self.remove = False\n",
    "\n",
    "        if self.strategy not in allowed_strategies:\n",
    "            raise ValueError(\"Can only use these strategies: {0} \"\n",
    "                             \" got strategy={1}\".format(allowed_strategies,\n",
    "                                                        self.strategy))\n",
    "\n",
    "        if self.axis not in [0, 1]:\n",
    "            raise ValueError(\"Can only impute missing values on axis 0 and 1, \"\n",
    "                             \" got axis={0}\".format(self.axis))\n",
    "\n",
    "        # Since two different arrays can be provided in fit(X) and\n",
    "        # transform(X), the imputation data will be computed in transform()\n",
    "        # when the imputation is done per sample (i.e., when axis=1).\n",
    "        if self.axis == 0:\n",
    "            X = check_array(X, accept_sparse='csc', dtype=np.float64,\n",
    "                            force_all_finite=False)      \n",
    "\n",
    "            if sparse.issparse(X):\n",
    "                self.statistics_ = self._sparse_fit(X, \n",
    "                                                    self.strategy,\n",
    "                                                    self.missing_values,\n",
    "                                                    self.axis)\n",
    "            else:\n",
    "                self.statistics_ = self._dense_fit(X, \n",
    "                                                   self.strategy,\n",
    "                                                   self.missing_values,\n",
    "                                                   self.axis)\n",
    "        return self\n",
    "\n",
    "    def _dense_fit(self, X, strategy, missing_values, axis):\n",
    "        \"\"\"Fit the transformer on dense data.\"\"\"\n",
    "\n",
    "        X = check_array(X, force_all_finite=False)\n",
    "        mask = _get_mask(X, missing_values)\n",
    "        masked_X = ma.masked_array(X, mask=mask)\n",
    "\n",
    "        # Mean\n",
    "        if strategy == \"mean\":\n",
    "            mean_masked = np.ma.mean(masked_X, axis=axis)\n",
    "            # Avoid the warning \"Warning: converting a masked element to nan.\"\n",
    "            mean = np.ma.getdata(mean_masked)\n",
    "            mean[np.ma.getmask(mean_masked)] = np.nan\n",
    "\n",
    "            return mean\n",
    "\n",
    "        # Median\n",
    "        elif strategy == \"median\":\n",
    "            if tuple(int(v) for v in np.__version__.split('.')[:2]) < (1, 5):\n",
    "                # In old versions of numpy, calling a median on an array\n",
    "                # containing nans returns nan. This is different is\n",
    "                # recent versions of numpy, which we want to mimic\n",
    "                masked_X.mask = np.logical_or(masked_X.mask,\n",
    "                                              np.isnan(X))\n",
    "            median_masked = np.ma.median(masked_X, axis=axis)\n",
    "            # Avoid the warning \"Warning: converting a masked element to nan.\"\n",
    "            median = np.ma.getdata(median_masked)\n",
    "            median[np.ma.getmaskarray(median_masked)] = np.nan\n",
    "\n",
    "            return median\n",
    "\n",
    "        # Most frequent\n",
    "        elif strategy == \"most_frequent\":\n",
    "            # scipy.stats.mstats.mode cannot be used because it will no work\n",
    "            # properly if the first element is masked and if its frequency\n",
    "            # is equal to the frequency of the most frequent valid element\n",
    "            # See https://github.com/scipy/scipy/issues/2636\n",
    "\n",
    "            # To be able access the elements by columns\n",
    "            if axis == 0:\n",
    "                X = X.transpose()\n",
    "                mask = mask.transpose()\n",
    "\n",
    "            most_frequent = np.empty(X.shape[0])\n",
    "\n",
    "            for i, (row, row_mask) in enumerate(zip(X[:], mask[:])):\n",
    "                row_mask = np.logical_not(row_mask).astype(np.bool)\n",
    "                row = row[row_mask]\n",
    "                most_frequent[i] = _most_frequent(row, np.nan, 0)\n",
    "\n",
    "            return most_frequent\n",
    "        \n",
    "        # Random: in fit: send values that can be selected\n",
    "        elif strategy == 'random':\n",
    "            \n",
    "            not_mask = mask == False\n",
    "            statistics = []         \n",
    "            for i in range(X.shape[1]):\n",
    "                statistics.append(X[:, i][not_mask[:, i]])\n",
    "            \n",
    "            return statistics #for each column a vector of possible imputations\n",
    "                \n",
    "        elif strategy in ['regression', 'stochastic']:\n",
    "                                     \n",
    "            X = X[~np.isnan(X).any(axis=1)] #how to do this by making use of mask/not_mask?            \n",
    "            statistics = []\n",
    "            \n",
    "            lr = LinearRegression()\n",
    "            for i in range(X.shape[1]):\n",
    "                dependent_var = X[:, i]\n",
    "                independent_var = np.delete(X, i, axis = 1)\n",
    "                lr.fit(independent_var, dependent_var)\n",
    "                stats_per_column = [lr.intercept_, lr.coef_]\n",
    "                \n",
    "                if strategy == 'stochastic':\n",
    "                    res = dependent_var - lr.predict(independent_var)\n",
    "                    sd = np.std(res)\n",
    "                    stats_per_column.append(sd)\n",
    "                    \n",
    "                statistics.append(stats_per_column)\n",
    "                \n",
    "            # this is the code from strategy == 'mean'\n",
    "            mean_masked = np.ma.mean(masked_X, axis=axis)\n",
    "            # Avoid the warning \"Warning: converting a masked element to nan.\"\n",
    "            mean = np.ma.getdata(mean_masked)\n",
    "            mean[np.ma.getmask(mean_masked)] = np.nan\n",
    "            statistics.append(mean)                       \n",
    "            \n",
    "            return statistics #for each column the fit of a regression line\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Impute all missing values in X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            The input data to complete.\n",
    "        \"\"\"      \n",
    "\n",
    "        if self.strategy in ['mean', 'median', 'most_frequent']:\n",
    "            \n",
    "            if self.axis == 0:\n",
    "                check_is_fitted(self, 'statistics_')\n",
    "                X = check_array(X, accept_sparse='csc', dtype=FLOAT_DTYPES, force_all_finite=False, copy=self.copy)\n",
    "                statistics = self.statistics_\n",
    "                \n",
    "                if X.shape[1] != statistics.shape[0]:\n",
    "                    raise ValueError(\"X has %d features per sample, expected %d\"\n",
    "                                     % (X.shape[1], self.statistics_.shape[0]))\n",
    "\n",
    "            # Since two different arrays can be provided in fit(X) and\n",
    "            # transform(X), the imputation data need to be recomputed\n",
    "            # when the imputation is done per sample\n",
    "            else:\n",
    "                X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES,\n",
    "                                force_all_finite=False, copy=self.copy)\n",
    "\n",
    "                if sparse.issparse(X):\n",
    "                    statistics = self._sparse_fit(X, \n",
    "                                                  self.strategy,\n",
    "                                                  self.missing_values,\n",
    "                                                  self.axis)\n",
    "\n",
    "                else:\n",
    "                    statistics = self._dense_fit(X, \n",
    "                                                 self.strategy,\n",
    "                                                 self.missing_values,\n",
    "                                                 self.axis)\n",
    "\n",
    "            # Delete the invalid rows/columns        \n",
    "            invalid_mask = np.isnan(statistics)\n",
    "            valid_mask = np.logical_not(invalid_mask)\n",
    "            valid_statistics = statistics[valid_mask]\n",
    "            valid_statistics_indexes = np.where(valid_mask)[0]\n",
    "            missing = np.arange(X.shape[not self.axis])[invalid_mask]\n",
    "\n",
    "            if self.axis == 0 and invalid_mask.any():\n",
    "                if self.verbose:\n",
    "                    warnings.warn(\"Deleting features without \"\n",
    "                                  \"observed values: %s\" % missing)\n",
    "                X = X[:, valid_statistics_indexes]\n",
    "            elif self.axis == 1 and invalid_mask.any():\n",
    "                raise ValueError(\"Some rows only contain \"\n",
    "                                 \"missing values: %s\" % missing)\n",
    "\n",
    "            # Do actual imputation\n",
    "            if sparse.issparse(X) and self.missing_values != 0:\n",
    "                mask = _get_mask(X.data, self.missing_values)\n",
    "                indexes = np.repeat(np.arange(len(X.indptr) - 1, dtype=np.int),\n",
    "                                    np.diff(X.indptr))[mask]\n",
    "\n",
    "                X.data[mask] = valid_statistics[indexes].astype(X.dtype,\n",
    "                                                            copy=False)\n",
    "            else:\n",
    "                if sparse.issparse(X):\n",
    "                    X = X.toarray()\n",
    "                mask = _get_mask(X, self.missing_values)\n",
    "                n_missing = np.sum(mask, axis=self.axis)\n",
    "                values = np.repeat(valid_statistics, n_missing)\n",
    "\n",
    "                if self.axis == 0:\n",
    "                    coordinates = np.where(mask.transpose())[::-1]\n",
    "                else:\n",
    "                    coordinates = mask\n",
    "\n",
    "                X[coordinates] = values\n",
    "                \n",
    "            if self.remove:\n",
    "                X = X[:, :-1]\n",
    "                          \n",
    "            return X            \n",
    "            \n",
    "        elif self.strategy == 'random':\n",
    "                    \n",
    "            if self.axis == 0:\n",
    "                check_is_fitted(self, 'statistics_')\n",
    "                X = check_array(X, accept_sparse='csc', dtype=FLOAT_DTYPES, force_all_finite=False, copy=self.copy)\n",
    "                statistics = self.statistics_       \n",
    "                \n",
    "            #Do actual imputation                  \n",
    "            if sparse.issparse(X):\n",
    "                X = X.toarray()\n",
    "            mask = _get_mask(X, self.missing_values)\n",
    "            n_missing = np.sum(mask, axis=self.axis)\n",
    "                     \n",
    "            values = np.array([])\n",
    "            for i in range(len(statistics)):\n",
    "                imputations = np.random.choice(statistics[i],size=n_missing[i],replace=True)\n",
    "                values = np.append(values, imputations)\n",
    "            \n",
    "            if self.axis == 0:\n",
    "                coordinates = np.where(mask.transpose())[::-1]\n",
    "            else:\n",
    "                coordinates = mask\n",
    "\n",
    "            X[coordinates] = values\n",
    "            if self.remove:\n",
    "                X = X[:, :-1]\n",
    "            \n",
    "            return X\n",
    "            \n",
    "        elif self.strategy in ['regression', 'stochastic']:\n",
    "                \n",
    "            if self.axis == 0:\n",
    "                check_is_fitted(self, 'statistics_')\n",
    "                X = check_array(X, accept_sparse='csc', dtype=FLOAT_DTYPES, force_all_finite=False, copy=self.copy)\n",
    "                statistics = self.statistics_  \n",
    "                mean_values = statistics[-1]\n",
    "                statistics = statistics[:-1]\n",
    "                \n",
    "            #Do actual imputation                  \n",
    "            if sparse.issparse(X):\n",
    "                X = X.toarray()\n",
    "            mask = _get_mask(X, self.missing_values)\n",
    "            \n",
    "            values = np.array([])\n",
    "            for i in range(len(statistics)):\n",
    "                               \n",
    "                dependent_var = X[:, i]                          \n",
    "                independent_var = np.delete(X[mask[:, i]], i, axis=1)\n",
    "                independent_mask = np.delete(mask[mask[:, i]], i, axis=1)\n",
    "                \n",
    "                # this is the code from mean imputation\n",
    "                # we need complete data to calculate the predictions\n",
    "                temp_n_missing = np.sum(independent_mask, axis=self.axis)\n",
    "                temp_mean_values = np.delete(mean_values, i)\n",
    "                temp_values = np.repeat(temp_mean_values, temp_n_missing)\n",
    "                coordinates = np.where(independent_mask.transpose())[::-1]\n",
    "                independent_var[coordinates] = temp_values\n",
    "                \n",
    "                imputations = np.dot(independent_var, statistics[i][1]) + statistics[i][0]\n",
    "                if self.strategy == 'stochastic':\n",
    "                    noise = np.random.normal(0, statistics[i][2], size = len(imputations))\n",
    "                    imputations = imputations + noise\n",
    "                    \n",
    "                values = np.append(values, imputations)\n",
    "            \n",
    "            if self.axis == 0:\n",
    "                coordinates = np.where(mask.transpose())[::-1]\n",
    "            else:\n",
    "                coordinates = mask\n",
    "\n",
    "            X[coordinates] = values\n",
    "            if self.remove:\n",
    "                X = X[:, :-1]\n",
    "            \n",
    "            return X  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class custom_pipeline(Pipeline):\n",
    "    def transform(self, X):\n",
    "        assert(isinstance(X,pd.DataFrame)), 'X needs to be pd.DataFrame'\n",
    "        return pd.DataFrame(super().transform(X),columns = X.columns, index=X.index)\n",
    "\n",
    "def create_custom_pipeline(imputationstrategy, model):\n",
    "    pipe = custom_pipeline([\n",
    "        ('imputer',imputationstrategy),\n",
    "        ('scaler',StandardScaler()),\n",
    "        ('model',model)])\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_mean_median(X_train,y_train,X_test,y_test,model,method='mean'):\n",
    "    X_train_use = X_train.copy()\n",
    "    X_test_use = X_test.copy()\n",
    "    pipe = create_custom_pipeline(imputationstrategy=custom_imputer(strategy=method),model=model)\n",
    "    mod = pipe.fit(X_train_use,y_train)\n",
    "    pred = mod.predict(X_test_use)\n",
    "    return {'mse': mse(y_test,pred),\n",
    "            'rmse': sqrt(mse(y_test,pred)),\n",
    "            'ev': ev(y_test,pred),\n",
    "            'mae': mae(y_test,pred),\n",
    "            'dif': mse(y_train,mod.predict(X_train_use)) - mse(y_test,pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_custom_imputation(X_train,y_train,X_test,y_test,model,method='random'):\n",
    "    X_train_use = X_train.copy()\n",
    "    X_test_use = X_test.copy()\n",
    "    pipe = create_custom_pipeline(imputationstrategy=custom_imputer(strategy=method),model=model)\n",
    "    mod = pipe.fit(X_train_use,y_train)\n",
    "    pred = mod.predict(X_test_use)\n",
    "    return {'mse': mse(y_test,pred),\n",
    "            'rmse': sqrt(mse(y_test,pred)),\n",
    "            'ev': ev(y_test,pred),\n",
    "            'mae': mae(y_test,pred),\n",
    "            'dif': mse(y_train,mod.predict(X_train_use)) - mse(y_test,pred)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perform_simulation(df,n_simulations,missing_proportions,output_variable):\n",
    "    results_mse = []\n",
    "    results_rmse = []\n",
    "    results_ev = []\n",
    "    results_mae = []\n",
    "    results_dif = []\n",
    "    for simulation in range(n_simulations):\n",
    "        print('busy with simulation ', simulation)\n",
    "                \n",
    "        train_ind, test_ind = apply_train_test(df)\n",
    "        train = df.loc[train_ind].copy()\n",
    "        test = df.loc[test_ind].copy()        \n",
    "        \n",
    "        for mechanism in ['MCAR','MARZ','MARX','MNARZ','MNARX']:\n",
    "            for proportion in missing_proportions:\n",
    "                \n",
    "                train_inc = delete_data(train,mechanism,output_variable,missing_row_proportion=proportion).\\\n",
    "                reset_index().drop('index',1)\n",
    "                test_inc = delete_data(test,mechanism,output_variable,missing_row_proportion=proportion).\\\n",
    "                reset_index().drop('index',1)          \n",
    "                \n",
    "                if mechanism == 'MARZ':\n",
    "                    X_train = train_inc.drop(output_variable,1)\n",
    "                    y_train = train_inc[output_variable]\n",
    "                    X_test = test_inc.drop(output_variable,1)\n",
    "                    y_test = test_inc[output_variable]\n",
    "                else:\n",
    "                    X_train = train_inc.drop([output_variable, 'z'],1)\n",
    "                    y_train = train_inc[output_variable]\n",
    "                    X_test = test_inc.drop([output_variable, 'z'],1)\n",
    "                    y_test = test_inc[output_variable]\n",
    "                \n",
    "                missing_cells_proportion = round(train.isnull().sum().mean()/len(train),3)\n",
    "                for name,model in [['lin',LinearRegression()]]: \n",
    "                    drop_ = apply_drop(train_inc,test_inc,X_train,y_train,X_test,y_test,model)\n",
    "                    mean_ = apply_mean_median(X_train,y_train,X_test,y_test,model,method='mean')\n",
    "                    median_ = apply_mean_median(X_train,y_train,X_test,y_test,model,method='median')\n",
    "                    random_ = apply_custom_imputation(X_train,y_train,X_test,y_test,model,method='random')   \n",
    "                    regression_ = apply_custom_imputation(X_train,y_train,X_test,y_test,model,method='regression')   \n",
    "                    stochastic_ = apply_custom_imputation(X_train,y_train,X_test,y_test,model,method='stochastic')  \n",
    "                                \n",
    "                    results_mse.append([simulation,mechanism,proportion,missing_cells_proportion,name,\n",
    "                                        drop_['mse'],mean_['mse'],median_['mse'],\n",
    "                                        random_['mse'],regression_['mse'], stochastic_['mse']])\n",
    "                    results_rmse.append([simulation,mechanism,proportion,missing_cells_proportion,name,\n",
    "                                        drop_['rmse'],mean_['rmse'],median_['rmse'],\n",
    "                                        random_['rmse'],regression_['rmse'], stochastic_['rmse']])\n",
    "                    results_mae.append([simulation,mechanism,proportion,missing_cells_proportion,name,\n",
    "                                       drop_['mae'],mean_['mae'],median_['mae'],\n",
    "                                       random_['mae'],regression_['mae'],stochastic_['mae']])\n",
    "                    results_ev.append([simulation,mechanism,proportion,missing_cells_proportion,name,\n",
    "                                       drop_['ev'],mean_['ev'],median_['ev'],\n",
    "                                       random_['ev'],regression_['ev'],stochastic_['ev']])\n",
    "                    results_dif.append([simulation,mechanism,proportion,missing_cells_proportion,name,\n",
    "                                       drop_['dif'],mean_['dif'],median_['dif'],\n",
    "                                       random_['dif'],regression_['dif'],stochastic_['dif']])\n",
    "    return results_mse, results_rmse, results_ev, results_mae, results_dif\n",
    "\n",
    "def save_results(results_mse, results_rmse, results_ev, results_mae, results_dif,nsimulations,missing_proportions):\n",
    "    export_mse = pd.DataFrame(results_mse,columns = ['simulation','missing_type','missing_rows_proportion',\n",
    "                                                     'missing_cells_proportion','model',\n",
    "                                                     'drop','mean','median','random','regression','stochastic'])\n",
    "    export_rmse = pd.DataFrame(results_rmse,columns = ['simulation','missing_type','missing_rows_proportion',\n",
    "                                                     'missing_cells_proportion','model',\n",
    "                                                     'drop','mean','median','random','regression','stochastic'])\n",
    "    export_ev = pd.DataFrame(results_ev,columns = ['simulation','missing_type','missing_rows_proportion',\n",
    "                                                   'missing_cells_proportion','model',\n",
    "                                                   'drop','mean','median','random','regression','stochastic'])\n",
    "    export_mae = pd.DataFrame(results_mae,columns = ['simulation','missing_type','missing_rows_proportion',\n",
    "                                                   'missing_cells_proportion','model',\n",
    "                                                   'drop','mean','median','random','regression','stochastic'])\n",
    "    export_dif = pd.DataFrame(results_dif,columns = ['simulation','missing_type','missing_rows_proportion',\n",
    "                                                   'missing_cells_proportion','model',\n",
    "                                                   'drop','mean','median','random','regression','stochastic'])\n",
    "    export = pd.concat([export_mse, export_rmse, export_ev, export_mae, export_dif])\n",
    "    rows_one_metric = n_simulations * len(missing_proportions) * 5\n",
    "    export['evaluation_metric'] = np.append(np.repeat('mse', rows_one_metric),\n",
    "                                            np.append(np.repeat('rmse', rows_one_metric),\n",
    "                                                      np.append(np.repeat('ev', rows_one_metric),\n",
    "                                                                np.append(np.repeat('mae', rows_one_metric),\n",
    "                                                                          np.repeat('dif', rows_one_metric)))))\n",
    "\n",
    "    group_dict = {}\n",
    "    for col in ['drop','mean','median','random', 'regression','stochastic']:\n",
    "        group_dict[col] = 'mean'\n",
    "        export[col+'_upper'] = export[col].copy()\n",
    "        export[col+'_lower'] = export[col].copy()\n",
    "        group_dict[col+'_lower'] = lambda x: np.percentile(x,25)\n",
    "        group_dict[col+'_upper'] = lambda x: np.percentile(x,75)\n",
    "    group_dict['missing_cells_proportion'] = 'mean'\n",
    "\n",
    "    export = export.groupby(['missing_type','missing_rows_proportion','model','evaluation_metric']).agg(group_dict)\n",
    "    \n",
    "    return export    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation with real dataset: Forest Fires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missing_proportions = np.linspace(0.05,0.55,20)\n",
    "n_simulations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X  Y  FFMC   DMC     DC  ISI  temp  RH  wind  rain  area\n",
       "0  7  5  86.2  26.2   94.3  5.1   8.2  51   6.7   0.0   0.0\n",
       "1  7  4  90.6  35.4  669.1  6.7  18.0  33   0.9   0.0   0.0\n",
       "2  7  4  90.6  43.7  686.9  6.7  14.6  33   1.3   0.0   0.0\n",
       "3  8  6  91.7  33.3   77.5  9.0   8.3  97   4.0   0.2   0.0\n",
       "4  8  6  89.3  51.3  102.2  9.6  11.4  99   1.8   0.0   0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.read_csv('Data/forest_fires.txt',sep='\\t')\n",
    "output_variable = 'area'\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "busy with simulation  0\n",
      "busy with simulation  1\n",
      "busy with simulation  2\n",
      "busy with simulation  3\n",
      "busy with simulation  4\n",
      "busy with simulation  5\n",
      "busy with simulation  6\n",
      "busy with simulation  7\n",
      "busy with simulation  8\n",
      "busy with simulation  9\n",
      "busy with simulation  10\n",
      "busy with simulation  11\n",
      "busy with simulation  12\n",
      "busy with simulation  13\n",
      "busy with simulation  14\n",
      "busy with simulation  15\n",
      "busy with simulation  16\n",
      "busy with simulation  17\n",
      "busy with simulation  18\n",
      "busy with simulation  19\n",
      "busy with simulation  20\n",
      "busy with simulation  21\n",
      "busy with simulation  22\n",
      "busy with simulation  23\n",
      "busy with simulation  24\n",
      "busy with simulation  25\n",
      "busy with simulation  26\n",
      "busy with simulation  27\n",
      "busy with simulation  28\n",
      "busy with simulation  29\n",
      "busy with simulation  30\n",
      "busy with simulation  31\n",
      "busy with simulation  32\n",
      "busy with simulation  33\n",
      "busy with simulation  34\n",
      "busy with simulation  35\n",
      "busy with simulation  36\n",
      "busy with simulation  37\n",
      "busy with simulation  38\n",
      "busy with simulation  39\n",
      "busy with simulation  40\n",
      "busy with simulation  41\n",
      "busy with simulation  42\n",
      "busy with simulation  43\n",
      "busy with simulation  44\n",
      "busy with simulation  45\n",
      "busy with simulation  46\n",
      "busy with simulation  47\n",
      "busy with simulation  48\n",
      "busy with simulation  49\n",
      "busy with simulation  50\n",
      "busy with simulation  51\n",
      "busy with simulation  52\n",
      "busy with simulation  53\n",
      "busy with simulation  54\n",
      "busy with simulation  55\n",
      "busy with simulation  56\n",
      "busy with simulation  57\n",
      "busy with simulation  58\n",
      "busy with simulation  59\n",
      "busy with simulation  60\n",
      "busy with simulation  61\n",
      "busy with simulation  62\n",
      "busy with simulation  63\n",
      "busy with simulation  64\n",
      "busy with simulation  65\n",
      "busy with simulation  66\n",
      "busy with simulation  67\n",
      "busy with simulation  68\n",
      "busy with simulation  69\n",
      "busy with simulation  70\n",
      "busy with simulation  71\n",
      "busy with simulation  72\n",
      "busy with simulation  73\n",
      "busy with simulation  74\n",
      "busy with simulation  75\n",
      "busy with simulation  76\n",
      "busy with simulation  77\n",
      "busy with simulation  78\n",
      "busy with simulation  79\n"
     ]
    }
   ],
   "source": [
    "results_mse, results_rmse, results_ev, results_mae, results_dif = \\\n",
    "perform_simulation(df_,n_simulations,missing_proportions,output_variable)\n",
    "export = save_results(results_mse, results_rmse, results_ev, results_mae, results_dif,n_simulations,missing_proportions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export.to_csv('Results/results_forest_fires.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation with real dataset: Slump Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.read_csv('Data/slump_test.txt',sep='\\t')\n",
    "output_variable = 'SLUMP(cm)'\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mse, results_rmse, results_ev, results_mae, results_dif = \\\n",
    "perform_simulation(df_,n_simulations,missing_proportions,output_variable)\n",
    "export = save_results(results_mse, results_rmse, results_ev, results_mae, results_dif,n_simulations,missing_proportions)\n",
    "export.to_csv('Results/results_slump_test.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation with real dataset: Red Wine Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.read_csv('Data/red_wine_quality.txt',sep='\\t')\n",
    "output_variable = 'quality'\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mse, results_rmse, results_ev, results_mae, results_dif = \\\n",
    "perform_simulation(df_,n_simulations,missing_proportions,output_variable)\n",
    "export = save_results(results_mse, results_rmse, results_ev, results_mae, results_dif,n_simulations,missing_proportions)\n",
    "export.to_csv('Results/results_red_wine_quality.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation with real dataset: School Alcohol Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.read_csv('Data/school_alcohol_consumption.txt',sep='\\t')\n",
    "output_variable = 'Dalc'\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mse, results_rmse, results_ev, results_mae, results_dif = \\\n",
    "perform_simulation(df_,n_simulations,missing_proportions,output_variable)\n",
    "export = save_results(results_mse, results_rmse, results_ev, results_mae, results_dif,n_simulations,missing_proportions)\n",
    "export.to_csv('Results/results_school_alcohol_consumption.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation with custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_simulations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom_dataset_poor_small\n",
    "df_ = pd.read_csv('Data/custom_dataset_poor_little.txt',sep='\\t')\n",
    "output_variable = 'y'\n",
    "results_mse, results_rmse, results_ev, results_mae, results_dif = perform_simulation(df_,\n",
    "                                                                        n_simulations,\n",
    "                                                                        missing_proportions,\n",
    "                                                                        output_variable)\n",
    "export = save_results(results_mse, results_rmse, results_ev, results_mae, results_dif,n_simulations,missing_proportions)\n",
    "export.to_csv('Results/results_custom_dataset_poor_little.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom_dataset_poor_large\n",
    "df_ = pd.read_csv('Data/custom_dataset_poor_much.txt',sep='\\t')\n",
    "output_variable = 'y'\n",
    "results_mse, results_rmse, results_ev, results_mae, results_dif = perform_simulation(df_,\n",
    "                                                                        n_simulations,\n",
    "                                                                        missing_proportions,\n",
    "                                                                        output_variable)\n",
    "export = save_results(results_mse, results_rmse, results_ev, results_mae, results_dif,n_simulations,missing_proportions)\n",
    "export.to_csv('Results/results_custom_dataset_poor_much.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom_dataset_rich_small\n",
    "df_ = pd.read_csv('Data/custom_dataset_rich_little.txt',sep='\\t')\n",
    "output_variable = 'y'\n",
    "results_mse, results_rmse, results_ev, results_mae, results_dif = perform_simulation(df_,\n",
    "                                                                        n_simulations,\n",
    "                                                                        missing_proportions,\n",
    "                                                                        output_variable)\n",
    "export = save_results(results_mse, results_rmse, results_ev, results_mae, results_dif,n_simulations,missing_proportions)\n",
    "export.to_csv('Results/results_custom_dataset_rich_little.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#custom_dataset_rich_large\n",
    "df_ = pd.read_csv('Data/custom_dataset_rich_much.txt',sep='\\t')\n",
    "output_variable = 'y'\n",
    "results_mse, results_rmse, results_ev, results_mae, results_dif = perform_simulation(df_,\n",
    "                                                                        n_simulations,\n",
    "                                                                        missing_proportions,\n",
    "                                                                        output_variable)\n",
    "export = save_results(results_mse, results_rmse, results_ev, results_mae, results_dif,n_simulations,missing_proportions)\n",
    "export.to_csv('Results/results_custom_dataset_rich_much.txt',sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
